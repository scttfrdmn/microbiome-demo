AWSTemplateFormatVersion: '2010-09-09'
Description: 'Microbiome Demo - Real-Time Progress Tracking Resources'

Parameters:
  DataBucketName:
    Type: String
    Description: 'Name of the S3 bucket for workflow data and progress tracking'
  NotificationEmail:
    Type: String
    Description: 'Email address for workflow notifications (optional)'
    Default: ''

Conditions:
  HasNotificationEmail:
    !Not [!Equals [!Ref NotificationEmail, '']]

Resources:
  # Lambda execution role with permissions for S3 and CloudWatch Logs
  ProgressLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: S3AccessPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:PutObject
                  - s3:ListBucket
                Resource:
                  - !Sub arn:aws:s3:::${DataBucketName}
                  - !Sub arn:aws:s3:::${DataBucketName}/*
        - PolicyName: SNSPublishPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - sns:Publish
                Resource:
                  - !GetAtt WorkflowNotificationTopic.TopicArn

  # Lambda function for progress notification
  ProgressNotificationFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: microbiome-progress-notification
      Handler: index.lambda_handler
      Role: !GetAtt ProgressLambdaRole.Arn
      Runtime: python3.8
      Timeout: 60
      MemorySize: 128
      Environment:
        Variables:
          DATA_BUCKET: !Ref DataBucketName
          SNS_TOPIC_ARN: !Ref WorkflowNotificationTopic
      Code:
        ZipFile: |
          import json
          import boto3
          import os
          import logging
          import time
          import traceback
          from datetime import datetime
          from botocore.exceptions import ClientError

          # Configure logging
          logger = logging.getLogger()
          logger.setLevel(logging.INFO)

          # Initialize AWS clients with error handling
          try:
              s3 = boto3.client('s3')
              sns = boto3.client('sns') if 'SNS_TOPIC_ARN' in os.environ else None
          except Exception as e:
              logger.error(f"Failed to initialize AWS clients: {str(e)}")
              # We'll handle this in the lambda_handler

          # Constants
          DEFAULT_STATUS = 'unknown'
          DEFAULT_PERCENT = 0
          DEFAULT_TIME_FORMAT = '--:--:--'
          MAX_RETRIES = 3
          BACKUP_SUFFIX = '.backup'

          class ProgressProcessingError(Exception):
              """Custom exception for progress processing errors"""
              pass

          def validate_event(event):
              """
              Validates the S3 event structure
              Returns tuple of (bucket, key) or raises ProgressProcessingError
              """
              if not event or 'Records' not in event or not event['Records']:
                  raise ProgressProcessingError("Invalid event structure - missing Records")
              
              record = event['Records'][0]
              if 's3' not in record or 'bucket' not in record['s3'] or 'object' not in record['s3']:
                  raise ProgressProcessingError("Invalid S3 event structure")
              
              try:
                  bucket = record['s3']['bucket']['name']
                  key = record['s3']['object']['key']
                  
                  if not bucket or not key:
                      raise ProgressProcessingError("Empty bucket name or key")
                      
                  return (bucket, key)
              except KeyError as e:
                  raise ProgressProcessingError(f"Missing required field in event: {str(e)}")

          def extract_workflow_id(key):
              """
              Extracts workflow ID from the key path
              Expected format: progress/{workflow_id}/progress.json
              """
              try:
                  parts = key.split('/')
                  if len(parts) >= 3:
                      return parts[1]
                  else:
                      logger.warning(f"Could not extract workflow ID from key: {key}")
                      return 'unknown'
              except Exception as e:
                  logger.warning(f"Error extracting workflow ID: {str(e)}")
                  return 'unknown'

          def get_progress_data(bucket, key, max_retries=MAX_RETRIES):
              """
              Retrieves and parses progress data from S3 with retry logic
              Returns dict or raises ProgressProcessingError
              """
              retry_count = 0
              last_exception = None
              
              while retry_count < max_retries:
                  try:
                      response = s3.get_object(Bucket=bucket, Key=key)
                      data = response['Body'].read().decode('utf-8')
                      
                      try:
                          return json.loads(data)
                      except json.JSONDecodeError as je:
                          # Try to recover corrupted JSON if possible
                          logger.warning(f"Error parsing JSON: {str(je)}. Attempting recovery...")
                          
                          # Check if backup exists
                          backup_key = f"{key}{BACKUP_SUFFIX}"
                          try:
                              backup_response = s3.get_object(Bucket=bucket, Key=backup_key)
                              backup_data = backup_response['Body'].read().decode('utf-8')
                              return json.loads(backup_data)
                          except Exception:
                              logger.warning("No valid backup found. Creating empty progress data.")
                              # Return empty but valid progress data
                              return {
                                  "status": "unknown",
                                  "percent_complete": 0,
                                  "processes": {
                                      "completed": 0,
                                      "total": 0
                                  }
                              }
                          
                  except ClientError as e:
                      error_code = e.response.get('Error', {}).get('Code', 'Unknown')
                      
                      # Don't retry if the object doesn't exist
                      if error_code == 'NoSuchKey':
                          raise ProgressProcessingError(f"Progress file does not exist: {key}")
                      
                      # For other client errors, retry
                      logger.warning(f"S3 client error (attempt {retry_count+1}/{max_retries}): {str(e)}")
                      last_exception = e
                      retry_count += 1
                      time.sleep(0.5 * retry_count)  # Exponential backoff
                      
                  except Exception as e:
                      logger.warning(f"Error getting progress data (attempt {retry_count+1}/{max_retries}): {str(e)}")
                      last_exception = e
                      retry_count += 1
                      time.sleep(0.5 * retry_count)  # Exponential backoff
              
              # If we've exhausted all retries
              if last_exception:
                  raise ProgressProcessingError(f"Failed to get progress data after {max_retries} attempts: {str(last_exception)}")
              else:
                  raise ProgressProcessingError(f"Unknown error getting progress data after {max_retries} attempts")

          def prepare_dashboard_data(progress_data, workflow_id):
              """
              Prepares dashboard data from progress data
              Uses safe getters with defaults for any missing data
              """
              # Create a dictionary with all expected fields to ensure dashboard doesn't break
              dashboard_data = {
                  'timestamp': int(time.time()),
                  'update_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                  'workflow_id': workflow_id,
                  'percent_complete': progress_data.get('percent_complete', DEFAULT_PERCENT),
                  'status': progress_data.get('status', DEFAULT_STATUS),
                  'elapsed_time': progress_data.get('elapsed_time_formatted', DEFAULT_TIME_FORMAT),
                  'remaining_time': progress_data.get('estimated_remaining_formatted', DEFAULT_TIME_FORMAT),
                  'start_time_human': progress_data.get('start_time_human', 'Not available'),
                  'processes': {
                      'completed': progress_data.get('completed_count', 0),
                      'total': progress_data.get('total_processes', 0),
                      'list': progress_data.get('processes', {})
                  }
              }
              
              # Validate percent complete is a number between 0-100
              try:
                  percent = float(dashboard_data['percent_complete'])
                  dashboard_data['percent_complete'] = max(0, min(100, percent))
              except (ValueError, TypeError):
                  dashboard_data['percent_complete'] = 0
                  
              # Validate status is a known value
              valid_statuses = ['waiting', 'running', 'completed', 'failed']
              if dashboard_data['status'] not in valid_statuses:
                  dashboard_data['status'] = DEFAULT_STATUS
                  
              return dashboard_data

          def update_dashboard(bucket, dashboard_data, workflow_id, max_retries=MAX_RETRIES):
              """
              Updates dashboard data files in S3 with retry logic
              Returns True on success or raises ProgressProcessingError
              """
              # Prepare JSON data
              try:
                  json_data = json.dumps(dashboard_data, indent=2)
                  content_type = 'application/json'
              except Exception as e:
                  raise ProgressProcessingError(f"Failed to serialize dashboard data: {str(e)}")
              
              # Define keys for workflow-specific and latest progress
              dashboard_key = f'dashboard/data/progress_{workflow_id}.json'
              latest_key = 'dashboard/data/latest_progress.json'
              keys_to_update = [dashboard_key, latest_key]
              
              # Update both files with retry logic
              for key in keys_to_update:
                  retry_count = 0
                  success = False
                  
                  while not success and retry_count < max_retries:
                      try:
                          # Create backup of existing file first if it exists
                          try:
                              existing = s3.get_object(Bucket=bucket, Key=key)
                              backup_key = f"{key}{BACKUP_SUFFIX}"
                              s3.put_object(
                                  Bucket=bucket,
                                  Key=backup_key,
                                  Body=existing['Body'].read(),
                                  ContentType=content_type
                              )
                          except ClientError:
                              # Object doesn't exist yet, no backup needed
                              pass
                              
                          # Update the file
                          s3.put_object(
                              Bucket=bucket,
                              Key=key,
                              Body=json_data,
                              ContentType=content_type
                          )
                          success = True
                          
                      except Exception as e:
                          logger.warning(f"Error updating dashboard file {key} (attempt {retry_count+1}/{max_retries}): {str(e)}")
                          retry_count += 1
                          if retry_count < max_retries:
                              time.sleep(0.5 * retry_count)  # Exponential backoff
                  
                  if not success:
                      raise ProgressProcessingError(f"Failed to update dashboard file {key} after {max_retries} attempts")
              
              logger.info(f"Dashboard data updated successfully at {dashboard_key} and {latest_key}")
              return True

          def send_notification(status, workflow_id, progress_data):
              """
              Sends SNS notification based on workflow status if SNS_TOPIC_ARN is configured
              """
              if not sns or 'SNS_TOPIC_ARN' not in os.environ:
                  logger.info("SNS notifications disabled - no SNS_TOPIC_ARN configured")
                  return False
                  
              topic_arn = os.environ['SNS_TOPIC_ARN']
              
              try:
                  # Only send notifications for completed or failed workflows
                  if status not in ['completed', 'failed']:
                      return False
                      
                  # Create notification message
                  subject = f"Microbiome Workflow {workflow_id} {status.capitalize()}"
                  
                  message = f"Workflow {workflow_id} {status}!\n\n"
                  message += f"Status: {status}\n"
                  
                  if status == 'completed':
                      message += f"Completed at: {progress_data.get('end_time_human', 'unknown')}\n"
                      message += f"Total runtime: {progress_data.get('total_runtime_formatted', 'unknown')}\n"
                  elif status == 'failed':
                      message += f"Failed at: {progress_data.get('end_time_human', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))}\n"
                      message += f"Error: {progress_data.get('error_message', 'Unknown error')}\n"
                  
                  message += f"\nSee dashboard for full results."
                  
                  # Send notification
                  response = sns.publish(
                      TopicArn=topic_arn,
                      Subject=subject,
                      Message=message
                  )
                  
                  logger.info(f"Sent {status} notification: {response['MessageId']}")
                  return True
                  
              except Exception as e:
                  logger.error(f"Failed to send {status} notification: {str(e)}")
                  return False

          def lambda_handler(event, context):
              """
              Lambda function to handle progress notifications from Nextflow workflow.
              
              This function is triggered by S3 events when progress files are updated.
              It processes the progress data and can send notifications or update dashboard data.
              """
              logger.info(f"Progress notification Lambda invoked")
              
              try:
                  # Log truncated event to prevent excessive log size
                  event_str = json.dumps(event)
                  logger.info(f"Received event: {event_str[:500]}{'...' if len(event_str) > 500 else ''}")
                  
                  # Validate and extract bucket and key
                  bucket, key = validate_event(event)
                  logger.info(f"Processing update from {bucket}/{key}")
                  
                  # Only process progress.json updates
                  if not key.endswith('progress.json'):
                      logger.info(f"Skipping non-progress file: {key}")
                      return {
                          'statusCode': 200,
                          'body': json.dumps('Skipped non-progress file')
                      }
                  
                  # Extract workflow ID from key
                  workflow_id = extract_workflow_id(key)
                  
                  # Get progress data
                  progress_data = get_progress_data(bucket, key)
                  
                  # Log progress information
                  status = progress_data.get('status', DEFAULT_STATUS)
                  percent = progress_data.get('percent_complete', DEFAULT_PERCENT)
                  logger.info(f"Workflow {workflow_id} progress: {percent}% complete, status: {status}")
                  logger.info(f"Elapsed: {progress_data.get('elapsed_time_formatted', DEFAULT_TIME_FORMAT)}, "
                            f"Remaining: {progress_data.get('estimated_remaining_formatted', DEFAULT_TIME_FORMAT)}")
                  
                  # Prepare and update dashboard data
                  dashboard_data = prepare_dashboard_data(progress_data, workflow_id)
                  update_dashboard(bucket, dashboard_data, workflow_id)
                  
                  # Send notification if workflow completed or failed
                  if status in ['completed', 'failed']:
                      send_notification(status, workflow_id, progress_data)
                      
                      # Publish a CloudWatch metric for workflow failures
                      if status == 'failed':
                          try:
                              cloudwatch = boto3.client('cloudwatch')
                              cloudwatch.put_metric_data(
                                  Namespace='Microbiome/Workflow',
                                  MetricData=[
                                      {
                                          'MetricName': 'WorkflowFailure',
                                          'Value': 1,
                                          'Unit': 'Count',
                                          'Dimensions': [
                                              {
                                                  'Name': 'WorkflowId',
                                                  'Value': workflow_id
                                              }
                                          ]
                                      }
                                  ]
                              )
                              logger.info(f"Published workflow failure metric for {workflow_id}")
                          except Exception as e:
                              logger.warning(f"Failed to publish failure metric: {str(e)}")
                  
                  return {
                      'statusCode': 200,
                      'body': json.dumps({
                          'message': 'Progress update processed successfully',
                          'workflow_id': workflow_id,
                          'status': status,
                          'percent_complete': percent
                      })
                  }
                  
              except ProgressProcessingError as e:
                  logger.error(f"Progress processing error: {str(e)}")
                  return {
                      'statusCode': 400,
                      'body': json.dumps({
                          'error': 'Progress processing error',
                          'message': str(e)
                      })
                  }
              except Exception as e:
                  # Log the full exception with stack trace for unexpected errors
                  logger.error(f"Unexpected error: {str(e)}")
                  logger.error(traceback.format_exc())
                  return {
                      'statusCode': 500,
                      'body': json.dumps({
                          'error': 'Internal server error',
                          'message': str(e)
                      })
                  }
      Tags:
        - Key: Project
          Value: microbiome-demo

  # S3 event notification configuration for progress updates
  ProgressBucketPermission:
    Type: AWS::Lambda::Permission
    Properties:
      Action: lambda:InvokeFunction
      FunctionName: !GetAtt ProgressNotificationFunction.Arn
      Principal: s3.amazonaws.com
      SourceArn: !Sub arn:aws:s3:::${DataBucketName}

  # SNS Topic for workflow notifications
  WorkflowNotificationTopic:
    Type: AWS::SNS::Topic
    Properties:
      DisplayName: Microbiome Workflow Notifications
      TopicName: microbiome-workflow-notifications

  # SNS Subscription (email) - only created if email is provided
  WorkflowNotificationSubscription:
    Type: AWS::SNS::Subscription
    Condition: HasNotificationEmail
    Properties:
      Protocol: email
      Endpoint: !Ref NotificationEmail
      TopicArn: !Ref WorkflowNotificationTopic
      
  # CloudWatch alarm for Lambda function errors
  LambdaErrorAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: Microbiome-Lambda-Errors
      AlarmDescription: Alarm when Lambda function has errors
      Namespace: AWS/Lambda
      MetricName: Errors
      Dimensions:
        - Name: FunctionName
          Value: !Ref ProgressNotificationFunction
      Statistic: Sum
      Period: 60
      EvaluationPeriods: 1
      Threshold: 1
      ComparisonOperator: GreaterThanOrEqualToThreshold
      TreatMissingData: notBreaching
      AlarmActions:
        - !Ref WorkflowNotificationTopic
        
  # Custom metric for workflow failures
  WorkflowFailureMetric:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: Microbiome-Workflow-Failure
      AlarmDescription: Alarm when workflow execution fails
      Namespace: Microbiome/Workflow
      MetricName: WorkflowFailure
      Statistic: Sum
      Period: 60
      EvaluationPeriods: 1
      Threshold: 1
      ComparisonOperator: GreaterThanOrEqualToThreshold
      TreatMissingData: notBreaching
      AlarmActions:
        - !Ref WorkflowNotificationTopic
        
  # Alarm for pipeline stalled (no progress updates for 15 minutes)
  PipelineStalledAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: Microbiome-Pipeline-Stalled
      AlarmDescription: Alarm when no pipeline progress updates for 15 minutes
      Namespace: AWS/S3
      MetricName: NumberOfObjects
      Dimensions:
        - Name: BucketName
          Value: !Ref DataBucketName
        - Name: StorageType
          Value: AllStorageTypes
      Statistic: Maximum
      Period: 300  # 5 minutes
      EvaluationPeriods: 3  # 15 minutes total
      Threshold: 0
      ComparisonOperator: LessThanOrEqualToThreshold
      TreatMissingData: breaching
      AlarmActions:
        - !Ref WorkflowNotificationTopic

Outputs:
  ProgressLambdaArn:
    Description: ARN of the progress notification Lambda function
    Value: !GetAtt ProgressNotificationFunction.Arn
  
  NotificationTopicArn:
    Description: ARN of the SNS notification topic
    Value: !GetAtt WorkflowNotificationTopic.TopicArn
  
  BucketName:
    Description: Name of the S3 bucket for progress tracking
    Value: !Ref DataBucketName
  
  # Instructions for setting up S3 notifications (can't be done directly in CloudFormation)
  SetupInstructions:
    Description: Instructions for setting up S3 event notifications
    Value: !Sub |
      After stack creation, set up S3 event notifications on ${DataBucketName} using:
      
      aws s3api put-bucket-notification-configuration \\
        --bucket ${DataBucketName} \\
        --notification-configuration '{
          "LambdaFunctionConfigurations": [
            {
              "LambdaFunctionArn": "${ProgressNotificationFunction.Arn}",
              "Events": ["s3:ObjectCreated:*"],
              "Filter": {
                "Key": {
                  "FilterRules": [
                    {
                      "Name": "prefix",
                      "Value": "progress/"
                    },
                    {
                      "Name": "suffix",
                      "Value": "progress.json"
                    }
                  ]
                }
              }
            }
          ]
        }'