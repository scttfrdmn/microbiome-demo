# Updated Microbiome Demo Implementation Plan

This document outlines the methodical approach to get the full microbiome pipeline working with real-time dashboard updates.

## Project Management Guidelines

### Commit Strategy
- **Commit Regularly**: Create focused commits after completing logical units of work
- **Descriptive Messages**: Use clear, concise commit messages that explain the why, not just the what
- **Group Related Changes**: Keep commits focused on a single feature or fix
- **Commit Structure**:
  ```
  <type>: <summary>

  <detailed description of changes>
  <explanation of why these changes were made>

  ðŸ¤– Generated with [Claude Code](https://claude.ai/code)
  Co-Authored-By: Claude <noreply@anthropic.com>
  ```
- **Types**: Use prefixes like `fix:`, `feat:`, `docs:`, `refactor:`, `test:`, `chore:`

### Testing Approach
- **Unit Tests**: Create tests for individual components
- **Integration Tests**: Test interactions between components
- **Mini-Tests**: Implement small validation tests after each significant change
- **Test Structure**:
  1. Setup test environment/data
  2. Run the component being tested
  3. Verify results match expectations
  4. Clean up test artifacts

## Current Status
- âœ… Fixed "nextflow: executable file not found in $PATH" error on ARM Graviton instances
- âœ… Created basic installer script and job definition
- âœ… Developed deployment and testing scripts
- âœ… Implemented dynamic resource optimization for ARM/x86 architectures and GPU/CPU availability
- âœ… Implemented real-time progress tracking in Nextflow workflow
- âœ… Created Lambda function for progress notifications and dashboard updates
- âœ… Implemented S3 website hosting for the dashboard
- âœ… Created real-time progress visualization dashboard with auto-refresh
- âœ… Developed comprehensive setup and testing scripts

## Phase 1: Complete ARM Compatibility Fix (Already Started)
- Finalize simple job definition for validation
  - ðŸ§ª **Mini-Test**: Run `test_graviton_fix.sh` to verify ARM compatibility
  - ðŸ’¾ **Commit**: `fix: complete ARM compatibility for Nextflow execution`
- Update scripts to ensure consistent ARM compatibility 
  - ðŸ§ª **Mini-Test**: Execute template rendering test with `bash -c "echo 'Template test' > workflow/templates/test.txt && cat workflow/templates/test.txt"`
  - ðŸ’¾ **Commit**: `fix: ensure consistent ARM compatibility in all scripts`
- Test end-to-end execution of Nextflow
  - ðŸ§ª **Mini-Test**: Run a simplified workflow with `./nextflow run workflow/microbiome_main.nf --help`
  - ðŸ’¾ **Commit**: `test: verify end-to-end Nextflow execution on ARM architecture`

## Phase 2: Full Pipeline Execution
- Implement direct S3-to-S3 transfer of reference databases
  - Create Lambda function to copy data directly from AWS Open Data Archive to the demo S3 bucket
    - ðŸ§ª **Mini-Test**: Test Lambda locally with `python -m unittest direct_reference_data_transfer_test.py`
    - ðŸ’¾ **Commit**: `feat: add Lambda function for direct S3-to-S3 data transfer`
  - Configure proper IAM roles for cross-bucket access permissions
    - ðŸ§ª **Mini-Test**: Verify permissions with `aws iam simulate-principal-policy`
    - ðŸ’¾ **Commit**: `chore: configure IAM roles for cross-bucket data transfer`
  - Ensure no data is downloaded to the user's computer during this process
    - ðŸ§ª **Mini-Test**: Run `setup_reference_data.sh --dry-run` to verify data path
    - ðŸ’¾ **Commit**: `feat: implement direct database access without local downloads`
  - Set up direct access to Kraken2 database from genome-idx S3 bucket
    - ðŸ§ª **Mini-Test**: Run `aws s3 ls s3://genome-idx/kraken/ --request-payer requester`
    - ðŸ’¾ **Commit**: `feat: add direct access to Kraken2 database from open data archive`
  - Configure MetaPhlAn and HUMAnN databases access directly from public sources
    - ðŸ§ª **Mini-Test**: Test database URL resolution with `curl -I <database-url>`
    - ðŸ’¾ **Commit**: `feat: configure direct access to MetaPhlAn and HUMAnN databases`
- Implement dynamic resource optimization for Nextflow on AWS Batch
  - Add initial resource detection process at workflow start
    - ðŸ§ª **Mini-Test**: Run `test_resource_detection.sh` on different instance types
    - ðŸ’¾ **Commit**: `feat: add resource detection process to Nextflow workflow`
  - Configure architecture-specific (ARM/x86) resource requirements
    - ðŸ§ª **Mini-Test**: Validate config with `nextflow config -profile test`
    - ðŸ’¾ **Commit**: `feat: implement architecture-specific resource profiles`
  - Implement process-specific resource allocation based on detected architecture
    - ðŸ§ª **Mini-Test**: Run simplified test process with `test_resource_allocation.sh`
    - ðŸ’¾ **Commit**: `feat: add dynamic resource allocation based on architecture`
  - Document resource requirements in configuration for user customization
    - ðŸ§ª **Mini-Test**: Verify documentation with `grep -r "resource" docs/`
    - ðŸ’¾ **Commit**: `docs: document resource optimization strategies and configuration`
- Create a production job definition that runs the full pipeline with access to these databases
  - ðŸ§ª **Mini-Test**: Validate job definition with `aws batch register-job-definition --cli-input-json file://job-definition.json --dry-run`
  - ðŸ’¾ **Commit**: `feat: create production job definition for full pipeline`
- Configure proper IAM roles for cross-service access to read from the open data archive
  - ðŸ§ª **Mini-Test**: Test permissions with AWS CLI policy simulator
  - ðŸ’¾ **Commit**: `chore: configure IAM roles for open data archive access`
- Test full pipeline execution with sample data
  - ðŸ§ª **Mini-Test**: Execute `test_full_pipeline.sh` with small sample dataset
  - ðŸ’¾ **Commit**: `test: verify full pipeline execution with sample data`

## Phase 3: Real-Time Progress Tracking (Implemented)
- âœ… Add progress tracking mechanism to Nextflow workflow (`beforeScript` and `afterScript` in each process)
  - ðŸ§ª **Mini-Test**: Verify hooks with `grep -r "beforeScript\|afterScript" workflow/`
  - ðŸ’¾ **Commit**: `feat: add progress tracking hooks to Nextflow workflow`
- âœ… Implement workflow.onComplete handler to publish final workflow status
  - ðŸ§ª **Mini-Test**: Test handler with `nextflow run workflow/microbiome_main.nf --help`
  - ðŸ’¾ **Commit**: `feat: implement workflow completion handler for status updates`
- âœ… Implement JSON-based progress metrics format with:
  - âœ… Elapsed time (actual runtime)
  - âœ… Process completion percentages
  - âœ… Accurate remaining time based on completed work
  - ðŸ§ª **Mini-Test**: Validate JSON format with `jq . test_progress.json`
  - ðŸ’¾ **Commit**: `feat: create comprehensive JSON progress metrics format`
- âœ… Create CloudFormation template for progress tracking resources
  - ðŸ§ª **Mini-Test**: Validate template with `aws cloudformation validate-template --template-body file://progress_tracking_cf.yaml`
  - ðŸ’¾ **Commit**: `infra: add CloudFormation template for progress tracking resources`
- âœ… Develop Lambda function to transform progress data for dashboard updates
  - ðŸ§ª **Mini-Test**: Test Lambda locally with SAM or Lambda test event
  - ðŸ’¾ **Commit**: `feat: implement Lambda function for progress data transformation`
- âœ… Configure S3 event notifications to trigger Lambda on progress updates
  - ðŸ§ª **Mini-Test**: Test notifications with `aws s3api put-bucket-notification-configuration --dry-run`
  - ðŸ’¾ **Commit**: `chore: configure S3 event notifications for progress updates`
- âœ… Create setup and test scripts for progress tracking
  - ðŸ§ª **Mini-Test**: Run `test_progress_tracking.sh` in dry-run mode
  - ðŸ’¾ **Commit**: `feat: add setup and test scripts for progress tracking system`

## Phase 4: Dashboard Integration (Implemented)
- âœ… Use S3 website hosting directly (no CloudFront) for faster deployment
  - ðŸ§ª **Mini-Test**: Validate S3 website configuration with `aws s3api get-bucket-website`
  - ðŸ’¾ **Commit**: `infra: configure S3 website hosting for dashboard`
- âœ… Configure proper S3 bucket CORS and website hosting settings
  - ðŸ§ª **Mini-Test**: Test CORS with `curl -I -X OPTIONS -H "Origin: http://example.com" <bucket-url>`
  - ðŸ’¾ **Commit**: `chore: set up CORS and website hosting for S3 bucket`
- âœ… Create real-time progress tracking dashboard with interactive visualizations
  - ðŸ§ª **Mini-Test**: Validate HTML with W3C validator or local browser testing
  - ðŸ’¾ **Commit**: `feat: implement real-time progress tracking dashboard`
- âœ… Implement automatic refresh using JavaScript (10 second intervals)
  - ðŸ§ª **Mini-Test**: Verify refresh with browser developer tools network monitoring
  - ðŸ’¾ **Commit**: `feat: add automatic refresh to dashboard`
- âœ… Add manual refresh capability with visual feedback
  - ðŸ§ª **Mini-Test**: Test manual refresh functionality in browser
  - ðŸ’¾ **Commit**: `feat: implement manual refresh with visual feedback`
- âœ… Create progress timeline and completion charts
  - ðŸ§ª **Mini-Test**: Validate chart rendering with test data
  - ðŸ’¾ **Commit**: `feat: add interactive progress charts to dashboard`
- âœ… Display process status with intuitive status indicators
  - ðŸ§ª **Mini-Test**: Test status indicators with different process states
  - ðŸ’¾ **Commit**: `feat: implement intuitive status indicators for processes`
- âœ… Ensure Lambda has direct write access to dashboard assets
  - ðŸ§ª **Mini-Test**: Verify Lambda permissions with policy simulator
  - ðŸ’¾ **Commit**: `chore: configure Lambda permissions for dashboard updates`
- âœ… Create setup and testing scripts for dashboard integration
  - ðŸ§ª **Mini-Test**: Run `test_dashboard_integration.sh` in dry-run mode
  - ðŸ’¾ **Commit**: `feat: add setup and test scripts for dashboard integration`

## Phase 5: Testing and Production Readiness
- Implement comprehensive error handling
  - Add try/catch blocks to all Lambda functions
    - ðŸ§ª **Mini-Test**: Test error handling with invalid inputs
    - ðŸ’¾ **Commit**: `fix: add comprehensive error handling to Lambda functions`
  - Implement graceful fallbacks in Nextflow processes
    - ðŸ§ª **Mini-Test**: Simulate failures and verify recovery
    - ðŸ’¾ **Commit**: `fix: implement graceful error handling in Nextflow processes`
  - Create error boundary for dashboard components
    - ðŸ§ª **Mini-Test**: Test dashboard with malformed data
    - ðŸ’¾ **Commit**: `fix: add error boundaries to dashboard components`
- Add alerting for pipeline failures
  - Configure SNS notifications for critical errors
    - ðŸ§ª **Mini-Test**: Trigger test notification with `aws sns publish`
    - ðŸ’¾ **Commit**: `feat: add SNS notifications for pipeline failures`
  - Implement CloudWatch alarms for workflow monitoring
    - ðŸ§ª **Mini-Test**: Validate alarm configuration with `aws cloudwatch describe-alarms`
    - ðŸ’¾ **Commit**: `feat: configure CloudWatch alarms for workflow monitoring`
  - Add dashboard alerts for failed processes
    - ðŸ§ª **Mini-Test**: Test alert rendering with simulated failures
    - ðŸ’¾ **Commit**: `feat: implement visual alerts for failed processes`
- Create detailed documentation
  - Write comprehensive README with architecture overview
    - ðŸ§ª **Mini-Test**: Validate instructions by following them on a clean environment
    - ðŸ’¾ **Commit**: `docs: create comprehensive README with architecture overview`
  - Document configuration options and customization
    - ðŸ§ª **Mini-Test**: Verify documentation by applying sample configurations
    - ðŸ’¾ **Commit**: `docs: add configuration and customization documentation`
  - Create troubleshooting guide
    - ðŸ§ª **Mini-Test**: Validate guide by resolving common issues
    - ðŸ’¾ **Commit**: `docs: add troubleshooting guide with common issues`
- Performance optimization for dashboard and pipeline
  - Optimize Lambda function execution time
    - ðŸ§ª **Mini-Test**: Benchmark Lambda performance with test events
    - ðŸ’¾ **Commit**: `perf: optimize Lambda function execution time`
  - Minimize dashboard network requests and payload size
    - ðŸ§ª **Mini-Test**: Analyze network performance with browser dev tools
    - ðŸ’¾ **Commit**: `perf: optimize dashboard network performance`
  - Implement resource-efficient Nextflow processes
    - ðŸ§ª **Mini-Test**: Compare resource usage before and after optimization
    - ðŸ’¾ **Commit**: `perf: optimize resource usage in Nextflow processes`

## Phase 6: Deployment and Maintenance
- Create master deployment script
  - Combine all setup scripts into a single master script
    - ðŸ§ª **Mini-Test**: Run deployment with `--dry-run` flag
    - ðŸ’¾ **Commit**: `feat: create comprehensive deployment script`
  - Add validation checks before deployment
    - ðŸ§ª **Mini-Test**: Test validation with invalid configurations
    - ðŸ’¾ **Commit**: `fix: add pre-deployment validation checks`
  - Create rollback mechanism for failed deployments
    - ðŸ§ª **Mini-Test**: Simulate deployment failure and verify rollback
    - ðŸ’¾ **Commit**: `feat: implement rollback mechanism for deployments`
- Implement versioning and updates
  - Add version tracking for deployed components
    - ðŸ§ª **Mini-Test**: Verify version tracking in deployed resources
    - ðŸ’¾ **Commit**: `chore: implement version tracking for components`
  - Create update script for seamless upgrades
    - ðŸ§ª **Mini-Test**: Test updates from previous to current version
    - ðŸ’¾ **Commit**: `feat: add seamless update mechanism`
  - Document upgrade procedures
    - ðŸ§ª **Mini-Test**: Follow upgrade documentation in test environment
    - ðŸ’¾ **Commit**: `docs: document upgrade procedures`
- Monitoring and maintenance
  - Setup resource utilization monitoring
    - ðŸ§ª **Mini-Test**: Verify CloudWatch metrics collection
    - ðŸ’¾ **Commit**: `feat: configure resource utilization monitoring`
  - Implement automated backups
    - ðŸ§ª **Mini-Test**: Test backup and restore functionality
    - ðŸ’¾ **Commit**: `feat: add automated backup system`
  - Create maintenance scripts for routine tasks
    - ðŸ§ª **Mini-Test**: Verify each maintenance script operation
    - ðŸ’¾ **Commit**: `feat: implement routine maintenance scripts`

## Implementation Notes
- The progress time elapsed and estimated time remaining should be based on real time from the start of the pipeline computation to its end
- Estimated time remaining should be calculated based on actual progress only from already completed work and the time it took
- Access to the web page's directory where updates are dropped should be accessible to the Lambdas directly
- No client-side hacks for transferring progress data
- Using S3 website hosting instead of CloudFront for faster demo deployment
- Use actual reference data from AWS open data archive - no placeholders or simulated data
- All data transfers must occur directly from AWS Open Data Archive to the demo S3 bucket (no transit through user's computer)
- Ensure proper IAM permissions for accessing the AWS open data resources
- Use AWS Lambda or Batch for any computation needed to prepare reference databases